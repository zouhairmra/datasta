import streamlit as st
from llama_cpp import Llama

# -------------------------------------------------
# 1Ô∏è‚É£ PAGE SETUP
# -------------------------------------------------
st.set_page_config(page_title="üß† Local LLaMA Economics Assistant", layout="centered")
st.title("ü¶ô Local LLaMA Economics Assistant")

# -------------------------------------------------
# 2Ô∏è‚É£ MODEL CONFIGURATION
# -------------------------------------------------
with st.expander("üß† Model Options"):
    model_path = st.text_input(
        "Enter path to your LLaMA model (.gguf):",
        value="models/phi-3-mini-4k-instruct.Q4_0.gguf"  # change to your model path
    )
    st.write(f"üîç Using model: **{model_path.split('/')[-1]}**")

with st.expander("üîß Advanced Settings"):
    temperature = st.slider("Temperature (creativity)", 0.0, 1.0, 0.7, 0.05)
    max_tokens = st.slider("Max tokens (response length)", 64, 2048, 512, 64)

# -------------------------------------------------
# 3Ô∏è‚É£ CHAT HISTORY
# -------------------------------------------------
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [
        {"role": "system", "content": "You are an expert in economics."}
    ]

for msg in st.session_state.chat_history[1:]:
    if msg["role"] == "user":
        st.markdown(f"üßë **You:** {msg['content']}")
    else:
        st.markdown(f"ü§ñ **Assistant:** {msg['content']}")

# -------------------------------------------------
# 4Ô∏è‚É£ USER INPUT
# -------------------------------------------------
user_input = st.text_area("üí¨ Your question:", height=150)

# -------------------------------------------------
# 5Ô∏è‚É£ GENERATE ANSWER
# -------------------------------------------------
if st.button("Generate Answer"):
    if not user_input.strip():
        st.error("‚ùå Please enter a question.")
    else:
        try:
            # Load LLaMA model
            st.write("‚è≥ Loading local model... (first time may take a few seconds)")
            llm = Llama(
                model_path=model_path,
                n_ctx=4096,
                n_threads=4,  # adjust based on your CPU
                verbose=False
            )

            # Add user message
            st.session_state.chat_history.append({"role": "user", "content": user_input.strip()})

            # Build conversation prompt
            messages = "\n".join(
                [f"{m['role'].capitalize()}: {m['content']}" for m in st.session_state.chat_history]
            ) + "\nAssistant:"

            # Generate response
            st.write("ü§ñ Generating response...")
            output = llm(
                messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stop=["User:", "Assistant:"]
            )

            response = output["choices"][0]["text"].strip()

            # Display and store
            st.markdown(f"ü§ñ **Assistant:** {response}")
            st.session_state.chat_history.append({"role": "assistant", "content": response})

        except Exception as e:
            st.error(f"‚ùå Error: {e}")
