import streamlit as st
from llama_index.llms.ollama import Ollama
from llama_index.core import VectorStoreIndex, Document
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.service_context import ServiceContext
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.chat_engine import CondenseQuestionChatEngine

# --- Sidebar ---
st.sidebar.title("ğŸ“š Datasta TutorBot")
st.sidebar.markdown("Ask me anything about Microeconomics or Business Math in Arabic!")

# --- Initialize only once ---
@st.cache_resource
def initialize_chat_engine():
    # Use Arabic-friendly embedding
    embed_model = HuggingFaceEmbedding(model_name="intfloat/multilingual-e5-base")

    # Local LLM from Ollama
    llm = Ollama(model="llama3")

    # Mini knowledge base
    content = """
    Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ Ø§Ù„Ø¬Ø²Ø¦ÙŠ Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ ÙŠØ±ÙƒØ² Ø¹Ù„Ù‰ Ø³Ù„ÙˆÙƒ Ø§Ù„Ø£ÙØ±Ø§Ø¯ ÙˆØ§Ù„Ø´Ø±ÙƒØ§Øª ÙÙŠ Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø¨Ø´Ø£Ù† ØªØ®ØµÙŠØµ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯. Ù…Ù† Ù…ÙØ§Ù‡ÙŠÙ…Ù‡: Ø§Ù„Ø¹Ø±Ø¶ ÙˆØ§Ù„Ø·Ù„Ø¨ØŒ ØªÙˆØ§Ø²Ù† Ø§Ù„Ø³ÙˆÙ‚ØŒ Ù…Ø±ÙˆÙ†Ø© Ø§Ù„Ø£Ø³Ø¹Ø§Ø±ØŒ ÙˆØªÙƒØ§Ù„ÙŠÙ Ø§Ù„Ø¥Ù†ØªØ§Ø¬.

    ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ù„Ù„Ø£Ø¹Ù…Ø§Ù„ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ù…ÙØ§Ù‡ÙŠÙ… Ù…Ø«Ù„ Ø§Ù„Ø¯ÙˆØ§Ù„ØŒ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ø®Ø·ÙŠØ©ØŒ Ø§Ù„ØªÙØ§Ø¶Ù„ØŒ ÙˆÙ†Ø³Ø¨ Ø§Ù„Ù†Ù…Ùˆ Ù„ÙÙ‡Ù… ÙˆØªØ­Ù„ÙŠÙ„ Ù…Ø³Ø§Ø¦Ù„ Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© Ù…Ø«Ù„ ØªØ¹Ø¸ÙŠÙ… Ø§Ù„Ø±Ø¨Ø­ Ø£Ùˆ ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ§Ù„ÙŠÙ.
    """

    documents = [Document(text=content)]
    parser = SimpleNodeParser()
    nodes = parser.get_nodes_from_documents(documents)

    service_context = ServiceContext.from_defaults(
        llm=llm,
        embed_model=embed_model
    )

    index = VectorStoreIndex(nodes, service_context=service_context)
    memory = ChatMemoryBuffer.from_defaults(token_limit=1500)

    return CondenseQuestionChatEngine.from_defaults(
        index=index,
        service_context=service_context,
        memory=memory,
        verbose=True,
    )

# --- Load engine ---
chat_engine = initialize_chat_engine()

# --- Chat UI ---
st.title("ğŸ¤– Microeconomics & Business Math Chatbot")
st.markdown("Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ Ø§Ù„Ø¬Ø²Ø¦ÙŠ Ø£Ùˆ Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Input box
user_input = st.chat_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§...")

# Display chat
for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Process user question
if user_input:
    st.session_state.chat_history.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        with st.spinner("Ø£ÙÙƒØ± ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©..."):
            response = chat_engine.chat(user_input)
            st.markdown(response.response)
            st.session_state.chat_history.append({"role": "assistant", "content": response.response})
