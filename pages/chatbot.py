import streamlit as st
from llama_index.llms.ollama import Ollama
from llama_index.core.chat_engine import CondenseQuestionChatEngine
from llama_index.core import VectorStoreIndex, SimpleNodeParser, Document
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.service_context import ServiceContext
from llama_index.core.memory import ChatMemoryBuffer

# Streamlit page config
st.set_page_config(page_title="ğŸ’¬ AI Tutor", layout="wide")
st.title("ğŸ“˜ğŸ¤– Tutor IA en Ã©conomie & mathÃ©matiques (arabe)")

# Load the model from Ollama (must be running locally)
llm = Ollama(model="llama3")

# Example knowledge base (you can customize this later)
documents = [
    Document(text="""
        Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ Ø§Ù„Ø¬Ø²Ø¦ÙŠ ÙŠØ¯Ø±Ø³ Ø³Ù„ÙˆÙƒ Ø§Ù„Ø£ÙØ±Ø§Ø¯ ÙˆØ§Ù„Ù…Ø¤Ø³Ø³Ø§Øª ÙÙŠ Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø­ÙˆÙ„ ØªØ®ØµÙŠØµ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù…Ø­Ø¯ÙˆØ¯Ø©. 
        Ù…ÙØ§Ù‡ÙŠÙ… Ø£Ø³Ø§Ø³ÙŠØ© ØªØ´Ù…Ù„: Ø§Ù„Ø¹Ø±Ø¶ ÙˆØ§Ù„Ø·Ù„Ø¨ØŒ Ø§Ù„ØªÙˆØ§Ø²Ù† ÙÙŠ Ø§Ù„Ø³ÙˆÙ‚ØŒ Ù…Ø±ÙˆÙ†Ø© Ø§Ù„Ø£Ø³Ø¹Ø§Ø±ØŒ Ø§Ù„ØªÙƒÙ„ÙØ© Ø§Ù„Ø­Ø¯ÙŠØ©ØŒ ÙˆØ§Ù„Ø¥ÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø¯ÙŠ.
        
        Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ù„Ù„Ø£Ø¹Ù…Ø§Ù„ ØªØ´Ù…Ù„: Ø§Ù„Ù†Ø³Ø¨ Ø§Ù„Ù…Ø¦ÙˆÙŠØ©ØŒ Ø§Ù„ÙØ§Ø¦Ø¯Ø© Ø§Ù„Ø¨Ø³ÙŠØ·Ø© ÙˆØ§Ù„Ù…Ø±ÙƒØ¨Ø©ØŒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒÙ„ÙØ© ÙˆØ§Ù„Ø¹Ø§Ø¦Ø¯ØŒ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ø®Ø·ÙŠØ©ØŒ ÙˆØ§Ù„Ù…ØµÙÙˆÙØ§Øª.
    """)
]

# Embedding + parser
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
parser = SimpleNodeParser()

# Vector index
nodes = parser.get_nodes_from_documents(documents)
service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)
index = VectorStoreIndex(nodes, service_context=service_context)

# Memory
memory = ChatMemoryBuffer.from_defaults(token_limit=1500)

# Chat engine
chat_engine = index.as_chat_engine(
    chat_mode="context",
    memory=memory,
    system_prompt="Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù„Ø´Ø±Ø­ Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ Ø§Ù„Ø¬Ø²Ø¦ÙŠ ÙˆØ§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø§Ù„ØªØ¬Ø§Ø±ÙŠØ© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„Ø·Ù„Ø§Ø¨.",
)

# Chat interface
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

user_input = st.chat_input("ğŸ“ Ø§Ø³Ø£Ù„ Ø¹Ù† Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ Ø§Ù„Ø¬Ø²Ø¦ÙŠ Ø£Ùˆ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ù„Ù„Ø£Ø¹Ù…Ø§Ù„")

if user_input:
    st.session_state.chat_history.append(("user", user_input))
    response = chat_engine.chat(user_input)
    st.session_state.chat_history.append(("ai", response.response))

# Show conversation
for role, msg in st.session_state.chat_history:
    if role == "user":
        st.chat_message("ğŸ§‘â€ğŸ“").markdown(msg)
    else:
        st.chat_message("ğŸ¤–").markdown(msg)
